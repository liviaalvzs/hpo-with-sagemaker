{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import boto3\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo os parâmetros para a base de dados\n",
    "np.random.seed(0)\n",
    "n_samples = 10000\n",
    "area = np.random.normal(150, 50, n_samples)\n",
    "num_rooms = np.random.randint(1, 6, n_samples)\n",
    "num_bathrooms = np.random.randint(1, 4, n_samples)\n",
    "age = np.random.randint(1, 50, n_samples)\n",
    "distance_to_city_center = np.random.normal(10, 5, n_samples)\n",
    "price = 100000 + (area * 1000) + (num_rooms * 5000) - (num_bathrooms * 3000) - (age * 1000) - (distance_to_city_center * 2000)\n",
    "\n",
    "# criando o dataframe (importante deixar \"price\" - variavel predita - na primeira posicao)\n",
    "data = pd.DataFrame({\n",
    "    'price': price,\n",
    "    'area': area,\n",
    "    'num_rooms': num_rooms,\n",
    "    'num_bathrooms': num_bathrooms,\n",
    "    'age': age,\n",
    "    'distance_to_city_center': distance_to_city_center\n",
    "})\n",
    "\n",
    "# salvando em csv \n",
    "data.to_csv('housing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Bases de Treino, Teste e Validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuração do bucket S3\n",
    "bucket = 'hpo-data'\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# normaliza os dados\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# converte de volta para DataFrame\n",
    "normalized_data = pd.DataFrame(normalized_data, columns=data.columns)\n",
    "\n",
    "# determina o número de linhas para cada conjunto (treino, teste e validacao)\n",
    "train_size = int(0.7 * len(normalized_data))\n",
    "test_size = int(0.2 * len(normalized_data))\n",
    "validation_size = len(normalized_data) - train_size - test_size\n",
    "\n",
    "# divide os dados nos tamanhos definidos acima\n",
    "train_data = normalized_data[:train_size]\n",
    "test_data = normalized_data[train_size:train_size + test_size]\n",
    "validation_data = normalized_data[train_size + test_size:]\n",
    "\n",
    "# salva os conjuntos de dados em arquivos CSV sem a header\n",
    "train_data.to_csv('houses_train_data.csv', index=False, header=False)\n",
    "test_data.to_csv('houses_test_data.csv', index=False, header=False)\n",
    "validation_data.to_csv('houses_validation_data.csv', index=False, header=False)\n",
    "\n",
    "# upload dos arquivos CSV para o bucket S3\n",
    "s3.upload_file('houses_train_data.csv', bucket, 'houses_train_data.csv')\n",
    "s3.upload_file('houses_test_data.csv', bucket, 'houses_test_data.csv')\n",
    "s3.upload_file('houses_validation_data.csv', bucket, 'houses_validation_data.csv')\n",
    "\n",
    "# caminhos S3 para os dados\n",
    "train_data_path = f's3://{bucket}/houses_train_data.csv'\n",
    "test_data_path = f's3://{bucket}/houses_test_data.csv'\n",
    "validation_data_path = f's3://{bucket}/houses_validation_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando Training Jobs (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuracao da entrada de dados para treinamento, teste e validação\n",
    "s3_input_train = TrainingInput(s3_data=train_data_path, content_type='text/csv')\n",
    "s3_input_validation = TrainingInput(s3_data=validation_data_path, content_type='text/csv')\n",
    "s3_input_test = TrainingInput(s3_data=test_data_path, content_type='text/csv')\n",
    "\n",
    "# obtém o papel IAM\n",
    "role = get_execution_role()\n",
    "\n",
    "# criacao do estimador SageMaker com o script de treinamento do XGBoost\n",
    "estimator = Estimator(entry_point='train_script.py',\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.m5.large',\n",
    "                      image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\"),\n",
    "                      sagemaker_session=sagemaker.Session())\n",
    "\n",
    "# hiperparametros para otimizacao\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "    'gamma': ContinuousParameter(0, 0.5),\n",
    "    'num_round': IntegerParameter(50, 500)\n",
    "}\n",
    "\n",
    "# configuracao da otimizacao de hiperparametros\n",
    "objective_metric_name = 'validation:rmse' \n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'validation:rmse', 'Regex': 'validation:rmse: ([0-9\\\\.]+)'}]\n",
    "\n",
    "# define numero de jobs + quantos serao executados simultaneamente\n",
    "max_jobs = 20\n",
    "max_parallel_jobs = 2\n",
    "\n",
    "# cria o tuner com a definicao da metrica\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            objective_type=objective_type,\n",
    "                            max_jobs=max_jobs,\n",
    "                            max_parallel_jobs=max_parallel_jobs)\n",
    "print(s3_input_test)\n",
    "\n",
    "# inicia a hpo e fornece os dados de treinamento, teste e validacao\n",
    "tuner.fit({'train': s3_input_train, 'test': s3_input_test, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo Melhor Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "# conecta ao sagemaker\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# lista os training jobs (os ultimos 20)\n",
    "response = sagemaker_client.list_training_jobs(\n",
    "    MaxResults=20,  # número maximo de jobs \n",
    "    SortBy='CreationTime',  # ordena por data de criacao\n",
    "    SortOrder='Descending'  # ordena em ordem decrescente para obter os jobs mais recentes primeiro\n",
    ")\n",
    "\n",
    "training_jobs = response['TrainingJobSummaries']\n",
    "\n",
    "# extrai infos relevantes dos jobs\n",
    "job_info = []\n",
    "for job in training_jobs:\n",
    "    job_name = job['TrainingJobName']\n",
    "    job_arn = job['TrainingJobArn']\n",
    "    job_status = job['TrainingJobStatus']\n",
    "    \n",
    "    # obtem metricas de desempenho se o job estiver concluido\n",
    "    if job_status == 'Completed':\n",
    "        metrics_response = sagemaker_client.describe_training_job(\n",
    "            TrainingJobName=job_name\n",
    "        )\n",
    "\n",
    "        # extrai a metrica de interesse (nesse caso, rMSE)\n",
    "        metric = metrics_response['FinalMetricDataList'][0]  # selecionando metricas (teste)\n",
    "        metric_value = metric['Value']\n",
    "\n",
    "        metric_V = metrics_response['FinalMetricDataList'][1]  # selecionando metricas (validacao)\n",
    "        metric_value_V = metric_V['Value']\n",
    "        \n",
    "        job_info.append({'JobName': job_name, 'rmseTrain': metric_value, 'rmseValidation': metric_value_V})\n",
    "\n",
    "# converte os resultados em um DataFrame pandas para facilitar a analise\n",
    "df = pd.DataFrame(job_info)\n",
    "\n",
    "# encontra melhor modelo com base na metrica\n",
    "best_model_train = df[df['rmseTrain'] == df['rmseTrain'].min()]\n",
    "best_model_validation = df[df['rmseValidation'] == df['rmseValidation'].min()]\n",
    "\n",
    "print(f'Melhor modelo (treino): {best_model_train}')\n",
    "print('')\n",
    "print(f'Melhor modelo (validacao): {best_model_validation}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
